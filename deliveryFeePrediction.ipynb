{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: 09/10/2019\n",
    "\n",
    "Environment: Python 3.7.3 and Jupyter notebook\n",
    "\n",
    "Libraries used: \n",
    "\n",
    "* pandas for providing a structure for data manipulation (for dataframe, included in Anaconda Python 3.7) \n",
    "* sklearn for data modelling\n",
    "* networx for calculating distances between nodes\n",
    "* numpy for manipulating pandas dataframe\n",
    "* datetime for handling date and time data\n",
    "* matplotlib and seaborn for plotting\n",
    "\n",
    "## Introduction\n",
    "\n",
    "1. Analysing a file containing missing data and imputing missing data. \n",
    "2. Analysing a file containing outliers and removing them. \n",
    "3. Analysing a file containing multiple data anomalies and fixing them. \n",
    "\n",
    "Methodology for each tasks is explained in more detail along with the task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value Task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task involves imputation of all missing values in CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following steps were taken in order to complete this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Analysis of input file with all its columns to identify null values in it.\n",
    "2. Identification of all relationships among all columns and selection of appropriate approach to impute those missing values.\n",
    "3. Impute missing values and write it to files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries \n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Reading the dataset with missing values and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'missing_data.csv' does not exist: b'missing_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2ea90eac23cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# reading the input missing file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_missing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'missing_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'missing_data.csv' does not exist: b'missing_data.csv'"
     ]
    }
   ],
   "source": [
    "# reading the input missing file\n",
    "df_missing = pd.read_csv('missing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dispalying first two records in dataframe\n",
    "df_missing.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-Graphical EDA is a way to describe the data without visualizing it. It can include functions such as .info(), .describe() and .shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking information of dataframe using non graphical EDA function i.e. info()\n",
    "df_missing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "We can see that the number of float64 data type columns are 5, int64 are 3 and object columns are 6. There are total 500 rows in each column except branch_code, delivery_time and distance column.\n",
    "- In branch-code 100 values are missing.\n",
    "- In delivery_fee 50 values are missing. \n",
    "- In distanceto_customer_KM 50 values are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Null Values\n",
    "\n",
    "#### Method:\n",
    "\n",
    "Each column can be tested by selecting rows where that column is null. The number of rows in output will indicate the amount of missing data for that column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values in column\n",
    "df_missing[df_missing['order_id'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "No null values found in order_id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values in column\n",
    "df_missing[df_missing['date'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "No null values found in date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values in column\n",
    "df_missing[df_missing['time'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "No null values found in time column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values in column\n",
    "df_missing[df_missing['order_type'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "No null values found in order type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values in column\n",
    "df_missing[df_missing['branch_code'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "Here we found 100 records where branch_code is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values in column\n",
    "df_missing[df_missing['order_items'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "No null values found in order_items column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values in column\n",
    "df_missing[df_missing['order_price'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations: \n",
    "\n",
    "No null values found in order_price column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values in column\n",
    "df_missing[df_missing['customer_lat'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "No null values found in customer latitude column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values in column\n",
    "df_missing[df_missing['customer_lon'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "No null values found in customer longitude column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values in column\n",
    "df_missing[df_missing['customerHasloyalty?'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "No null values found in customerHasloyalty column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values in column\n",
    "df_missing[df_missing['distance_to_customer_KM'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the shape of column, which gives number of records and number of column in dataframe\n",
    "df_missing[df_missing['distance_to_customer_KM'].isnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "Here we found 50 records where distance_to_customer_KM is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values in column\n",
    "df_missing[df_missing['delivery_fee'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the shape of column, which gives number of records and number of column in dataframe\n",
    "df_missing[df_missing['delivery_fee'].isnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "Here we found 50 records in missing data where delivery fee is missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for combination of columns with missing data:\n",
    "\n",
    "##### 3 columns\n",
    "\n",
    "Now we are checking are there any rows where all three values are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for null values in same row for three columns\n",
    "df_missing[df_missing['branch_code'].isnull() & df_missing['distance_to_customer_KM'].isnull() & df_missing['delivery_fee'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations: \n",
    "\n",
    "So there are no rows where all three values are null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2 columns\n",
    "\n",
    "Similarly, we are checking are there any two rows where both the values are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for null values in same row for two columns\n",
    "df_missing[df_missing['branch_code'].isnull() & df_missing['distance_to_customer_KM'].isnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "Here we found, there are 50 rows where branch code and distance_to_customer_KM both are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for null values in same row for two columns\n",
    "df_missing[df_missing['distance_to_customer_KM'].isnull() & df_missing['delivery_fee'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "There are no rows where distance to customer Km and delivery fee is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for null values in same row for two columns\n",
    "df_missing[df_missing['branch_code'].isnull() &  df_missing['delivery_fee'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "There are no rows where branch code and delivery fee is missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are dealing with branch code column where 100 values are missing. We will fix this using the relationship exist in order id and branch code as discussed later while auditing dirty data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows which have missing branch code is subset into a new dataframe named df_branchcode_missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying null value records in new dataframe and displaying records from it.\n",
    "df_branchcode_missing = df_missing[df_missing['branch_code'].isnull()]\n",
    "df_branchcode_missing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing Branch code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing the Branch code by the same method observed in Dirty data. Please refer into the Dirty Data section for more details. While the code is slightly different because it has been coded by two different people, the methodology is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for fixing branch code \n",
    "for index,row in df_branchcode_missing['order_id'].iteritems():\n",
    "    o_id = row.split('ORD')\n",
    "    var1 = o_id[1].startswith('C') or o_id[1].startswith('I') or o_id[1].startswith('Z')\n",
    "    var2 = o_id[1].startswith('K') or o_id[1].startswith('X') or o_id[1].startswith('A')\n",
    "    var3 = o_id[1].startswith('J') or o_id[1].startswith('Y') or o_id[1].startswith('B')\n",
    "    if var1 is True:\n",
    "        df_branchcode_missing.loc[index,'branch_code'] = 'NS'\n",
    "    if var2 is True:\n",
    "        df_branchcode_missing.loc[index,'branch_code'] = 'BK'\n",
    "    if var3 is True:\n",
    "        df_branchcode_missing.loc[index,'branch_code'] = 'TP'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using non graphical EDA displaying information of dataframe after imputing branch_code\n",
    "df_branchcode_missing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result: \n",
    "\n",
    "Now there are 100 values in branch_code column which was null earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining missing dataframe and main dataframe together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for concatenating dataframes\n",
    "df_missing = pd.concat([df_missing, df_branchcode_missing])\n",
    "df_missing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now it is giving us duplicate value. So here we are removing all duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for removing duplication in dataframe\n",
    "df_missing = df_missing.drop_duplicates(subset=['order_id',], keep='last', inplace=False)\n",
    "df_missing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "Here we can see new count after fixing branch code column. Now we are getting 500 count for branch count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance to customer KM\n",
    "\n",
    "Now we are reading 3 input files which are necessary to impute missing distance distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading input node.csv file\n",
    "df_nodes = pd.read_csv('nodes.csv')\n",
    "df_nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading input edges.csv file\n",
    "df_edges = pd.read_csv('edges.csv')\n",
    "df_edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading input node.csv file\n",
    "df_branches = pd.read_csv('branches.csv')\n",
    "df_branches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing distance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method:\n",
    "\n",
    "We are using dijkstra's algorithm to find the shortest distance between cusomer and branch. Here we are using customer latitude and longitude to find the nearest node from node.csv file and we are getting second node from branches.csv file. We used networkx library to calculate this distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are calculating shortest distance using dijkstra's algorithm (networkx package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate nearest node\n",
    "def nearest_node(input_list, input_value):\n",
    "    array = np.asarray(input_list)\n",
    "    id_val = (np.abs(input_list - input_value)).argmin()\n",
    "    return input_list[id_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for calculating the nearest node from given latitude and longitude\n",
    "edge_list = list(zip(df_edges.u, df_edges.v, df_edges['distance(m)']))\n",
    "node_list = list(zip(df_nodes.node))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(node_list)\n",
    "G.add_weighted_edges_from(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the shortest distance between branch and customer in \\\n",
    "# KM for records containing missing distance.\n",
    "\n",
    "def check_dist (row):\n",
    "    if (str(row['distance_to_customer_KM'])=='nan') == False:\n",
    "        return row['distance_to_customer_KM']\n",
    "    else:\n",
    "        lat_value1 = nearest_node(df_nodes['lat'],row['customer_lat'])\n",
    "        lon_value1 = nearest_node(df_nodes['lon'],row['customer_lon'])\n",
    "        ref_lat1 = list(df_nodes.loc[df_nodes['lat'] == lat_value1].node)\n",
    "        ref_lon1 = list(df_nodes.loc[df_nodes['lon'] == lon_value1].node)\n",
    "    \n",
    "\n",
    "        node_value = [x for x in ref_lon1 if x in ref_lat1]\n",
    "\n",
    "        for j in range(len(df_branches)):\n",
    "            if row['branch_code'].upper() == df_branches.branch_code[j]:\n",
    "                lat_value2 = df_branches.branch_lat[j]\n",
    "                lon_value2 = df_branches.branch_lon[j]\n",
    "                ref_lat2 = list(df_nodes.loc[df_nodes['lat'] == lat_value2].node)\n",
    "                ref_lon2 = list(df_nodes.loc[df_nodes['lon'] == lon_value2].node)\n",
    "                node_final = [x for x in ref_lon2 if x in ref_lat2]\n",
    "\n",
    "                distance = nx.dijkstra_path_length(G, node_value[0], node_final[0])/1000\n",
    "    \n",
    "                return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying above function to relevant data. \n",
    "\n",
    "df_missing['distance_to_customer_KM'] = df_missing.apply(check_dist,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculated shortest distance between customer and branch node and imputed where it was missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to display info of dataframe using non graphical EDA info() function\n",
    "df_missing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:\n",
    "\n",
    "Now we can see count of distance_to_customer_KM become 500 which was 400 before imputing in it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing delivery fee "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix delivery fee we are using linear regression model. We know that delivery fee linearly depends on following things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. weekend or weekday (1 or 0) - as a continuous variable\n",
    "\n",
    "b. time of the day (morning 0, afternoon 1, evening 2) - as a continuous variable\n",
    "\n",
    "c. distance between branch and customer\n",
    "\n",
    "Further, \n",
    "\n",
    "d. If a customer has loyalty, they get a 50% discount on delivery fee\n",
    "\n",
    "e. Each branch uses a different linear relationship with the attributes listed above. \n",
    "\n",
    "\n",
    "Methodology:\n",
    "\n",
    "A linear model model will be created using, a,b and c above as continuous attributes. Customer loyalty status will also be included in the model. Dummy variables will be created for all three branches and included in the model to handle the categorical variable. (Another option was to create a different model for each branch, however, this would severly limit the number of data points available for training and testing the model.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are generating extra columns as required in same dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for creating day_time column\n",
    "time_list1 = ['Breakfast','Lunch','Dinner']\n",
    "df_missing['day_time'] = df_missing['order_type'].apply(lambda x : time_list1.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for crreatiing weekday_weekday column\n",
    "df_missing['weekday_weekend'] = df_missing['date'].apply(lambda x : 0 if datetime.strptime(x, '%Y-%m-%d').weekday() <= 5 else 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(df_missing.branch_code)\n",
    "df_missing = df_missing.join(dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying records from dataframe after adding 2 required columns\n",
    "df_missing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see we have added 5 columns one is day-time which tells time of the day (morning 0, afternoon 1, evening 2) - as a continuous variable ,second is weekday_weekend which tells weekend or weekday (1 or 0) - as a continuous variable and 3 dumy columns one for each branch code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before generating the model, we need to remove the rows with missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to removing null values from dataframe\n",
    "training_data = df_missing.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing the data into training and test set to ensure that the model does not overfit the training data and it works well for unknown data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using train_test_split function from sklearn to divide training and test set. \n",
    "# Using 40% of data for test and 60% for training. \n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = \\\n",
    "train_test_split(training_data[['distance_to_customer_KM','customerHasloyalty?','day_time',\\\n",
    "                                'weekday_weekend','NS','BK','TP']],\\\n",
    "                 training_data['delivery_fee'], test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are intializing linear regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to intialize LinearRegression() model\n",
    "lr_model = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are fitting training data in all three models(Branch wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to fit training data in model\n",
    "lr_model.fit(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking R2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher the accuracy score, better the linear relationship in among variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to check accuracy score of model\n",
    "lr_model.score(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.score(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "Our model has a score of >88% on both training and test data. The test score is higher than training score, suggesting that the model is good at generalising and works well for unknown data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are predicting for delivery fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans_df = lambda df_missing: df_missing[df_missing.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = nans_df(df_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_pred = pred_data[['distance_to_customer_KM','customerHasloyalty?','day_time','weekday_weekend','NS','BK','TP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to predict delivery fee for each branch\n",
    "lr_model_predict = lr_model.predict(Xtest_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are assigning all predicted values to its respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to assign predicted delivery fee to respective dataframe\n",
    "pred_data['delivery_fee'] = lr_model_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are concatenating missing value dataframes and original dataframe into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to concat dataframes\n",
    "df_missing_fixed = pd.concat([pred_data,df_missing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non graphical EDA function to display info of dataframe\n",
    "df_missing_fixed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, values are getting duplicated so we are removing duplications from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to remove duplicates from dataframe\n",
    "df_missing_fixed = df_missing_fixed.drop_duplicates(subset=['order_id',], keep='first', inplace=False)\n",
    "df_missing_fixed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are getting all values imputed and getting 500 count for all records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to check shape of dataframe which displays number of records and number of columns in it\n",
    "df_missing_fixed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we had 12 columns in dataset so here we are removing 5 extra columns which we dont need in final output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to delete dataframe columns\n",
    "del df_missing_fixed['weekday_weekend']\n",
    "del df_missing_fixed['day_time']\n",
    "del df_missing_fixed['BK']\n",
    "del df_missing_fixed['NS']\n",
    "del df_missing_fixed['TP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to check the shape of dataframe after deleting 5 dataframe columns\n",
    "df_missing_fixed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export CSV file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas exporting dataframe in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to export dataframe into CSV file format\n",
    "df_missing_fixed.to_csv(\"missing_data_solution.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task involves detection and deletion of all outliers in CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following steps were taken in order to complete this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Finding residual using linear regression\n",
    "\n",
    "2. Following steps were taken in order to complete this task.\n",
    "\n",
    "3. removing all respective rows where outliers detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the outlier dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading input outlier file\n",
    "df_outlier = pd.read_csv('outlier_data.csv')\n",
    "df_outlier.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating data for outliers\n",
    "\n",
    "matplotlib package is used to show any outliers in dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plotting outlier dataframe using boxplot\n",
    "plt.figure(figsize=(14,5),dpi = 100)\n",
    "df_outlier.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the ranges of all columns are different so we are not able to analyze clearly for delivery fee so here we are plotting seperate boxplot for delivery fee using same package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for plotting boxplot for delivery fee column\n",
    "df_outlier.boxplot(column='delivery_fee')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "It is clearly visible that delivery column has so many outliers in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the model training on missing data for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create 5 required columns in dataframe\n",
    "dummies = pd.get_dummies(df_outlier.branch_code)\n",
    "df_outlier = df_outlier.join(dummies)\n",
    "time_list1 = ['Breakfast','Lunch','Dinner']\n",
    "df_outlier['day_time'] = df_outlier['order_type'].apply(lambda x : time_list1.index(x))\n",
    "df_outlier['weekday_weekend'] = df_outlier['date'].apply(lambda x : 0 if datetime.strptime(x, '%Y-%m-%d').weekday() <= 5 else 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating outlier data into dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create testing dataset\n",
    "Xtest_out = df_outlier[['distance_to_customer_KM','customerHasloyalty?','day_time','weekday_weekend', 'BK','NS','TP']]\n",
    "Ytest_out = df_outlier['delivery_fee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Here we are predicting for delivery fee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to predict delivery fee\n",
    "lr_model_predict = lr_model.predict(Xtest_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving all predicted value in dataframe by creating new column in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to assign predicted delivery fee to new column in dataframe\n",
    "df_outlier['predicted_delivery_fee'] =  lr_model_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating residual value here by using the formula Residual = predicted delivery fee - actual delivery fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to find residuals by using formula predicted - actual value\n",
    "df_outlier['Residual_value'] = df_outlier['predicted_delivery_fee'] - df_outlier['delivery_fee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverting residual values to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to convert residual value in numeric \n",
    "df_outlier['Residual_value'] = pd.to_numeric(df_outlier['Residual_value'])\n",
    "df_outlier.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method:\n",
    "Function for detecting Outliers. Here interquantile range is used to detect outlier. First we calculated 3rd quartile(75%) and first quartile(25%) and then calculated Interquartile range with formula q3-q1. then calculated the boundaries for outliers. Low boundary with formula q1 - 1.5* IQR and high boundary with formula q3 + 1.5*IQR. Whatever elements which are not in these boundaries we detected them as outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for detecting outliers in data\n",
    "def detect_outlier(df_outlier):\n",
    "    q1 = df_outlier['Residual_value'].quantile(0.25)\n",
    "    q3 = df_outlier['Residual_value'].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    df_only_outlier = df_outlier[(df_outlier['Residual_value'] < fence_low) | (df_outlier['Residual_value'] > fence_high)]\n",
    "#     print(len(df_only_outlier['Residual_value']))\n",
    "    return df_only_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function call and displaying outlier list\n",
    "outlier_df = detect_outlier(df_outlier)\n",
    "list(outlier_df['Residual_value'])\n",
    "print('Here we found %d outliers and the outliers are : '%len(list(outlier_df['Residual_value'])))\n",
    "print(list(outlier_df['Residual_value']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the outliers we detected in outlier dataframe. Now we are finding respective records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to find respective records where outliers found\n",
    "outlier_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see these are the records where we found outlier in delivery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are removing outlier records from original dataframe to get data without outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to display records without outliers\n",
    "\n",
    "df_without_outliers = df_outlier[~df_outlier['order_id'].isin(list(outlier_df['order_id']))]\n",
    "df_without_outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are getting dataframe after removing outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we had 12 columns in dataset so here we are removing all extra columns which we dont need in final output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to remove columns from dataframe\n",
    "del df_without_outliers['weekday_weekend']\n",
    "del df_without_outliers['day_time']\n",
    "del df_without_outliers['Residual_value']\n",
    "del df_without_outliers['predicted_delivery_fee']\n",
    "del df_without_outliers['BK']\n",
    "del df_without_outliers['NS']\n",
    "del df_without_outliers['TP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export CSV File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas exporting final resulting dataframe in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to export dataframe to CSV file format\n",
    "df_without_outliers.to_csv(\"outlier_data_solution.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Dirty Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction: \n",
    "\n",
    "We have been provided with a dataset containing various errors. The task is to perform an exploratory data analysis to audit data for the errors found in it and then fixing it. The way we have approached this problem is to explore each column against the known requirements given in the specification file and fixing errors as we go. We keep track of each error found and summarize it at the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandas read_csv function to read data.\n",
    "\n",
    "df_dirty = pd.read_csv('dirty_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading other required files that are provided along with the dataset. \n",
    "\n",
    "df_edges = pd.read_csv('edges.csv')\n",
    "df_nodes = pd.read_csv('nodes.csv')\n",
    "df_branches = pd.read_csv('branches.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "We begin by having a high level look at the dataset to then make some initial observations that may be useful later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to print data shape. \n",
    "print(\"The data set labelled 'dirty_data' contains \",df_dirty.shape[0], \\\n",
    "      \" records for \",df_dirty.shape[1], \"variables.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at first few lines of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using head() to look at the first few rows.\n",
    "\n",
    "df_dirty.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- Order ID has the format 'ORD' which is likely a short form for order. This is followed by a single letter and then a five digit number. While the number does not seem to be significant, the letter could be related to some attribute.  Both letter and the numbers can be investigated for a possible relationship to then perhaps correct any discrepancy, if found.\n",
    "- Date column does not look uniform in its format. The date is meant to be in YYYY-MM-DD format, however date for order-id ORDA10928 is 2018-17-07. There cannot be a 17th month in a year. \n",
    "- Time is in 24hr format. \n",
    "- Three branch codes are visible in the first 5 records. (NS, BK, TP)\n",
    "\n",
    "#### Categorical variables\n",
    "\n",
    "- Nominal: order_id, branch_code, order_items (The quantity related to each item is discrete-numerical), customerHasloyalty?\n",
    "- Ordinal: order_type\n",
    "\n",
    "#### Numerical variables\n",
    "\n",
    "- Continuous: date(can also considered discrete), time, order_price, customer_lat, customer_lon, distance_to_customer_KM, delivery_fee\n",
    "- Discrete: Quantity of order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using info() to look at information on variables.  \n",
    "\n",
    "df_dirty.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- date is type object, should be a datetime type. \n",
    "- time is type object, should be a datetime type. \n",
    "- customerHasloyalty? is type int64, should be bool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using describe() to look at data distribution. \n",
    "\n",
    "df_dirty.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- customer_lat has a maximum value of 145.009858 and customer_lon has a minimum value of -37.816282, it appears that the values for latitude and longitude are swapped for some records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dirty.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- order_id has all unique values, this is consistent with database rules.\n",
    "- branch_code has 6 values instead of 3. \n",
    "- combination of items in order_items is largely unique only 6 records have non-unique combinations. Given that the maximum frequency is 2, it seems like there are 3 combinations that repeat once. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for a detailed investigation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating order items into its components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval function separated out the list into tuples. \n",
    "\n",
    "df_dirty['order_items'] = df_dirty.order_items.apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the total types of items ordered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dirty[\"num_items\"] = df_dirty.order_items.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the combined ordered items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = list(df_dirty.columns)\n",
    "index_list.remove('order_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the list of ordered items and then creating a separate record for each item.\n",
    "# Please refer to the reference at the end for details on the method used in this code. \n",
    "items = df_dirty.order_items.apply(pd.Series) \\\n",
    ".merge(df_dirty, left_index = True, right_index = True) \\\n",
    ".drop([\"order_items\"], axis = 1) \\\n",
    ".melt(id_vars = index_list, value_name = \"order_item\") \\\n",
    ".drop(\"variable\", axis = 1) \\\n",
    ".dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting order item into two separate columns, one containing item name and the other quantity\n",
    "\n",
    "items['item'] = items.order_item.str[0]\n",
    "items['quantity'] = items.order_item.str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any orders contain only one item\n",
    "\n",
    "items.sort_values(\"num_items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "All items contain a minimum of two distinct items. So it will not be possible to get the price for each individual item directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the letter code from order id for further investigation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['id_code']  = items.order_id.apply(lambda x: x[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a column to track errors in records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[\"error_found\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating all variables individually\n",
    "\n",
    "#### Order ID\n",
    "\n",
    "- Must be unique: Given that there are 500 records, there should be 500 unique values in order_id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking for null values\n",
    "\n",
    "any(items.order_id.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(items.order_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "There are no null values and each record has a unique id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date\n",
    "\n",
    "- must be in the format YYYY-MM-DD\n",
    "- Check records for which, date format is not compliant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking for null values\n",
    "\n",
    "any(items.date.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function for checking date format. \n",
    "\n",
    "def check_date(date_string, date_format):\n",
    "    try:\n",
    "        date_obj = datetime.strptime(date_string, date_format)\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the above function to the date in each row and \n",
    "# adding a column to record rows that contain error in date format. \n",
    "\n",
    "items[\"date_error\"] = items.date.apply(lambda x: check_date(x,'%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(items.order_id[items.date_error == True].unique())), \" records have error in the date format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# records containing date error. \n",
    "\n",
    "items[items.date_error == True].sort_values(\"order_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating all errors for date errors. \n",
    "\n",
    "items.error_found[items.date_error == True] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "37 records have incorrect date format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by parsing date through the pd.to_datetime function with dayfirst = True. \n",
    "# All dates are converted into expected format. \n",
    "items['date'] = pd.to_datetime(items['date'], dayfirst=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time\n",
    "\n",
    "- must be in format hh:mm:ss\n",
    "- check records for which, time format is not compliant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking for null values\n",
    "\n",
    "any(items.time.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for checking time format. \n",
    "\n",
    "def check_time(time_string, time_format):\n",
    "    try:\n",
    "        time_obj = datetime.strptime(time_string, time_format)\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying above function and adding column time error\n",
    "\n",
    "items[\"time_error\"] = items.time.apply(lambda x: check_time(x,'%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(items.order_id[items.time_error == True].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting time to a time object. \n",
    "\n",
    "items['time'] = items['time'].apply(lambda x: datetime.strptime(x, '%H:%M:%S').time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "No time errors were observed. The data type was incorrect which has been fixed above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### order_type\n",
    "\n",
    "- There are only three types of orders and their type is determined by the time of order:\n",
    "    - Breakfast - served during morning (8am - 12pm),\n",
    "    - Lunch - served during afternoon (12:00:01pm - 4pm)\n",
    "    - Dinner - served during evening (4:00:01pm - 8pm)\n",
    "- Check records for which, order type is not compliant for above rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking for null values\n",
    "\n",
    "any(items.order_type.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking if there are any other types listed other than the ones alowed. \n",
    "\n",
    "items.order_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the allowed types are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for Checking if order type labels are correctly assigned based on order time rules.\n",
    "\n",
    "def check_type (row):\n",
    "    if  \\\n",
    "    ((datetime.strptime('08:00:00','%H:%M:%S').time() <= row['time'] <= datetime.strptime('12:00:00','%H:%M:%S').time())\\\n",
    "    & (row['order_type'] == 'Breakfast')) | \\\n",
    "    ((datetime.strptime('12:00:00','%H:%M:%S').time() < row['time'] <= datetime.strptime('16:00:00','%H:%M:%S').time())\\\n",
    "    & (row['order_type'] == 'Lunch')) | \\\n",
    "    ((datetime.strptime('16:00:00','%H:%M:%S').time() < row['time'] <= datetime.strptime('20:00:00','%H:%M:%S').time())\\\n",
    "    & (row['order_type'] == 'Dinner')):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the above function and recording records containing errors into a type_error column\n",
    "\n",
    "items[\"type_error\"] = items.apply(check_type,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(items.order_id[items.type_error == True].unique())), \" records contain errors for order type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# updating total errors with order type errors. \n",
    "\n",
    "items.error_found[items.type_error == True] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# records with order type error. \n",
    "items[items.type_error == True].sort_values(\"order_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing Order Type\n",
    "\n",
    "Diven that there are no errors in time, time can be used to find out the correct classification of order type based on given rules. The incorrect records are then corrected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for correcting records with order type errors. \n",
    "\n",
    "def fix_type (row):\n",
    "    if (datetime.strptime('08:00:00','%H:%M:%S').time() <= row['time'] <= datetime.strptime('12:00:00','%H:%M:%S').time()):\n",
    "        return \"Breakfast\"\n",
    "    if (datetime.strptime('12:00:00','%H:%M:%S').time() < row['time'] <= datetime.strptime('16:00:00','%H:%M:%S').time()):\n",
    "        return \"Lunch\"\n",
    "    if (datetime.strptime('16:00:00','%H:%M:%S').time() < row['time'] <= datetime.strptime('20:00:00','%H:%M:%S').time()):\n",
    "        return \"Dinner\"\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['order_type'] = items.apply(fix_type,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### branch_code\n",
    "\n",
    "- There are three branches\n",
    "- In order to check if the correct code is assigned to a branch, we need to find out if there is a relationship between branch code and any other variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any(items.branch_code.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No null branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking relationship between branch code and the letter identifier of order_id.\n",
    "\n",
    "pd.crosstab(items.branch_code,items.id_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a strong relationship between the id_code and branch_code. Letters A,K and X seem to be associated with branch BK, letters C, I and Z seem to be associated with branch NS and letters B, J and Y seem to be associated with branch TP. Also some branch codes are in lower case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for checking branchcode usingrelationship with order id. \n",
    "def check_branch (row):\n",
    "    if  (((row['id_code'] in ['A','K','X']) & (row['branch_code'] == 'BK')) | \\\n",
    "    ((row['id_code'] in ['C','I','Z']) & (row['branch_code'] == 'NS'))| \\\n",
    "    ((row['id_code'] in ['B','J','Y']) & (row['branch_code'] == 'TP'))):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[\"branch_error\"] = items.apply(check_branch,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(items.order_id[items.branch_error == True].unique())), \" records contain an error for the branch code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# updating total errors with branch code error. \n",
    "\n",
    "items.error_found[items.branch_error == True] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items containing branch code error. \n",
    "\n",
    "items[items.branch_error == True].sort_values(\"order_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing branch_code\n",
    "\n",
    "Branch code can be fixed using the relationship discoevered between order id and branch code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fix branch code. \n",
    "\n",
    "def fix_branch (row):\n",
    "    if  (row['id_code'] in ['A','K','X']):\n",
    "        return 'BK'\n",
    "    if  (row['id_code'] in ['C','I','Z']):\n",
    "        return 'NS'\n",
    "    if  (row['id_code'] in ['B','J','Y']):\n",
    "        return 'TP'\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "items['branch_code'] = items.apply(fix_branch,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### order_items\n",
    "\n",
    "- We need to check if there is any error in menu items. \n",
    "- We need to ensure that items ordered for breakfast, lunch and dinner come from distinct menus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of order items. \n",
    "items.item.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All menu items seem to be unique without any spelling or case errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the relationship of order items with order type\n",
    "pd.crosstab(items.order_type,items.item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All items with majority in an order type seem to fit into that type, if the same item is listed in a different order type, it must be an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing correct items in appropriate menu lists\n",
    "\n",
    "breakfast_menu = ['Cereal','Coffee','Eggs','Pancake']\n",
    "dinner_menu = ['Fish&Chips','Pasta','Salmon','Shrimp']\n",
    "lunch_menu = ['Burger','Chicken','Fries','Salad','Steak']\n",
    "\n",
    "# creating a function for checking if a record contains incorrect item.\n",
    "\n",
    "def check_item (row):\n",
    "    if  (((row['item'] in breakfast_menu) & (row['order_type'] == 'Breakfast')) | \\\n",
    "    ((row['item'] in lunch_menu) & (row['order_type'] == 'Lunch'))| \\\n",
    "    ((row['item'] in dinner_menu) & (row['order_type'] == 'Dinner'))):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "items[\"item_error\"] = items.apply(check_item,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(items.order_id[items.item_error == True].unique())), \" records contain an error in item type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating total errors with item error. \n",
    "items.error_found[items.item_error == True] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# records with item error\n",
    "items[items.item_error == True].sort_values(\"order_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### order_price\n",
    "\n",
    "- We know that each record, does not contain more than one error. This means that all records that contain date_error or time_error or type_error or branch error have the correct order_items and order_price. \n",
    "\n",
    "- This way we can collect all ordered items, their quantity and total price and solve these as a system of linear equations to get the price of each individual menu items. This can then be used to find pricing errors in the remaining records.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total records available to solve this problem. \n",
    "len(items.order_id[(items.date_error == True) | (items.time_error == True) | (items.branch_error == True)].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the records that do not contain errors in item type \n",
    "# or price because they are already known to contain other errors. \n",
    "price_data_df = items[(items.date_error == True) | (items.time_error == True) | (items.branch_error == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data_id = list(price_data_df.order_id[(items.date_error == True) | (items.time_error == True) | (items.branch_error == True)].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a list of unique order ids in this data. \n",
    "price_data_id.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a list of items that we need to get the price for. \n",
    "item_list = list(price_data_df.item.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a coefficient matrix for solving linear equations. \n",
    "coeff_matrix = np.zeros((len(price_data_id),len(item_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a dependent matrix for solving linear equations. \n",
    "depend_matrix = np.zeros((len(price_data_id),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding quantity of items as coefficients for the items and the total order price as the dependent value. \n",
    "for index, row in price_data_df.iterrows():\n",
    "    coeff_matrix[price_data_id.index(row['order_id']),item_list.index(row['item'])] = row['quantity']\n",
    "    depend_matrix[price_data_id.index(row['order_id'])] = row['order_price']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using linalg for solving the system of linear equations. \n",
    "price_list = list(np.around(np.array(list(np.linalg.lstsq(coeff_matrix,depend_matrix,rcond=None)[0])),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating coefficient matrix to check for accuracy\n",
    "for i in range(0,len(price_list)):\n",
    "    coeff_matrix[:,i] *= price_list[i]        \n",
    "coeff_matrix = coeff_matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking price accuracy\n",
    "np.around(depend_matrix-coeff_matrix,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All zeros mean that the equations were solved completely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using all data to find orders with pricing error. \n",
    "full_data_id = list(items.order_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_id.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same method as above to find records with pricing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_coeff_matrix = np.zeros((len(full_data_id),len(item_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_price_matrix = np.zeros((len(full_data_id),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in items.iterrows():\n",
    "    full_coeff_matrix[full_data_id.index(row['order_id']),item_list.index(row['item'])] = row['quantity']\n",
    "    full_price_matrix[full_data_id.index(row['order_id'])] = row['order_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(price_list)):\n",
    "    full_coeff_matrix[:,i] *= price_list[i]        \n",
    "full_coeff_matrix = full_coeff_matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pricing_err_list = list(np.around(full_price_matrix-full_coeff_matrix,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pricing_err_id = []\n",
    "for i in range(0,len(pricing_err_list)):\n",
    "    if pricing_err_list[i] != 0.0:\n",
    "        pricing_err_id.append(full_data_id[i])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any order where the total price calculated above is not equal to the recorded price is considered to contain an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[\"pricing_error\"] = items.order_id.apply(lambda x: True if x in pricing_err_id else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(items.order_id[items.pricing_error == True].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "items.error_found[items.pricing_error == True] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[items.pricing_error == True].sort_values(\"order_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fixing order_items\n",
    "\n",
    "From above we know the price of each item. For each record containing an incorrect item, the item can be fixed by looking up the price that was charged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of menu price. \n",
    "menu_price = dict(zip(item_list,price_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating total price\n",
    "def price_calc (row):\n",
    "    return menu_price[row['item']]*row['quantity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df of records \n",
    "fixitem_df = items[items.order_id.isin(list(items.order_id[items.item_error == True]\\\n",
    "                                            .unique()))].sort_values('order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating cost for the item. \n",
    "fixitem_df['item_cost'] = fixitem_df.apply(price_calc,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe for orders containing correct items. \n",
    "correct_item = fixitem_df[fixitem_df.item_error == False]\\\n",
    "[['order_id','order_type','order_price','item','quantity','item_error','item_cost']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe for orders containing incorrect items. \n",
    "incorrect_item = fixitem_df[fixitem_df.item_error == True]\\\n",
    "[['order_id','order_type','order_price','item','quantity','item_error','item_cost']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the correct totals. \n",
    "correct_item_total = correct_item.groupby(['order_id','order_price']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "incorrect_item = incorrect_item.drop(['order_price','item_error','item_cost'],axis = 1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_item_total = correct_item_total.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding difference in pricing error\n",
    "correct_item_total['err_price'] = correct_item_total.order_price - correct_item_total.item_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correct_item_total = correct_item_total.drop(['quantity','item_error','item_cost'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe to be used for finding item\n",
    "find_item = pd.merge(incorrect_item,correct_item_total,on='order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost for correct item per piece\n",
    "find_item['cost_per_piece'] =   round(find_item.err_price/find_item.quantity,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to find correct item using price\n",
    "menu_item = dict(zip(price_list,item_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finiding correct item\n",
    "find_item['correct_item'] = find_item.cost_per_piece.apply(lambda x: menu_item[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_dict = dict(zip(find_item.order_id,find_item.correct_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fix item error\n",
    "def fix_item (row):\n",
    "    if row[\"item_error\"] == True:\n",
    "        return item_dict[row[\"order_id\"]]\n",
    "    else:\n",
    "        return row[\"item\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[\"item\"] = items.apply(fix_item,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing order_price\n",
    "\n",
    "Order price can be fixed by calculating the correct price for orders using items, quantity and cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for fixing price. \n",
    "fixprice_df = items[items.order_id.isin(list(items.order_id[items.pricing_error == True]\\\n",
    "                                            .unique()))].sort_values('order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding item cost\n",
    "fixprice_df['item_cost'] = fixprice_df.apply(price_calc,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order total\n",
    "correct_price_total = fixprice_df.groupby(['order_id','order_price']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary of orderids and correct order price. \n",
    "price_dict = dict(zip(correct_price_total.order_id,correct_price_total.item_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add correct price\n",
    "def fix_price (row):\n",
    "    if row[\"pricing_error\"] == True:\n",
    "        return price_dict[row[\"order_id\"]]\n",
    "    else:\n",
    "        return row[\"order_price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[\"order_price\"] = items.apply(fix_price,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### customer_lat and customer_lon\n",
    "\n",
    "- Given that all locations should be within a tight geographical area, any outliers in latitude and longitude could be an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.boxplot(column=['customer_lat','customer_lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### observations\n",
    "\n",
    "- all outliers for customer_lat are >-25 and all outliers for customer_lon are <125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[(items.customer_lat > -25) | (items.customer_lon <125)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### observations\n",
    "\n",
    "- There appears to be two types of errors\n",
    "    - one where the latitude values are positive instead of negative. \n",
    "    - second where values for latitude and longitude are swapped around. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colecting orderids for orders that contain coordinate error. \n",
    "items[\"coord_error\"] = \\\n",
    "items.order_id.apply(lambda x: \\\n",
    "                     True if x in list(items.order_id[(items.customer_lat > -25) | (items.customer_lon <125)].unique())\\\n",
    "                     else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(items.order_id[items.coord_error == True].unique())), \"records contain error in co ordinates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "items.error_found[items.coord_error == True] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[items.coord_error == True].sort_values(\"order_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing Customer Latitutde and Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a second copy of latitude and longitude. \n",
    "items['lon'] = items['customer_lon']\n",
    "items['lat'] = items['customer_lat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for swapping incorrect lon with lat\n",
    "def fix_lon (row):\n",
    "    if row['customer_lon'] < 0:\n",
    "        return row['lat']\n",
    "    else:\n",
    "        return row['lon']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for swapping incorrect lat with lon and also fixing the signage\n",
    "def fix_lat (row):\n",
    "    if 0 < row['customer_lat'] < 50:\n",
    "        return -row['lat']\n",
    "    elif row['customer_lat'] > 50:\n",
    "        return row['lon']\n",
    "    else:\n",
    "        return row['lat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['customer_lon'] = items.apply(fix_lon,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['customer_lat'] = items.apply(fix_lat,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.boxplot(column=['customer_lat','customer_lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distance_to_customer_KM\n",
    "\n",
    "As seen previously, correct distance to customer can be found using the dijkstra algorithm, records containing distance errors will not match the calculated distances. All comments for the following cose are available above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_node(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = list(zip(df_edges.u, df_edges.v, df_edges['distance(m)']))\n",
    "node_list = list(zip(df_nodes.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(node_list)\n",
    "G.add_weighted_edges_from(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dist (row):\n",
    "    if row['error_found'] == True:\n",
    "        return row['distance_to_customer_KM']\n",
    "    else:\n",
    "        lat_value1 = nearest_node(df_nodes['lat'],row['customer_lat'])\n",
    "        lon_value1 = nearest_node(df_nodes['lon'],row['customer_lon'])\n",
    "        ref_lat1 = list(df_nodes.loc[df_nodes['lat'] == lat_value1].node)\n",
    "        ref_lon1 = list(df_nodes.loc[df_nodes['lon'] == lon_value1].node)\n",
    "    \n",
    "\n",
    "        node_value = [x for x in ref_lon1 if x in ref_lat1]\n",
    "\n",
    "        for j in range(len(df_branches)):\n",
    "            if row['branch_code'].upper() == df_branches.branch_code[j]:\n",
    "                lat_value2 = df_branches.branch_lat[j]\n",
    "                lon_value2 = df_branches.branch_lon[j]\n",
    "                ref_lat2 = list(df_nodes.loc[df_nodes['lat'] == lat_value2].node)\n",
    "                ref_lon2 = list(df_nodes.loc[df_nodes['lon'] == lon_value2].node)\n",
    "                node_final = [x for x in ref_lon2 if x in ref_lat2]\n",
    "\n",
    "                distance = nx.dijkstra_path_length(G, node_value[0], node_final[0])/1000\n",
    "    \n",
    "                return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['distance_to_customer_KM_test'] = items.apply(check_dist,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[\"dist_diff\"] = np.around(items.distance_to_customer_KM,3)-np.around(items.distance_to_customer_KM_test,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['dist_error'] = items.dist_diff.apply(lambda x: True if (np.around(x,3) != 0.000) else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.error_found[items.dist_error == True] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[items.dist_error == True][['order_id','customer_lat','customer_lon','distance_to_customer_KM','distance_to_customer_KM_test','dist_diff','dist_error']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing distance_to_customer_KM\n",
    "\n",
    "Records where incorrect distances are recorded are corrected with calculated distances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dist (row):\n",
    "    if row['dist_error'] == True:\n",
    "        return row['distance_to_customer_KM_test']\n",
    "    else:\n",
    "        return row['distance_to_customer_KM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['distance_to_customer_KM'] = items.apply(fix_dist,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customer has Loyalty\n",
    "\n",
    "- Loyal customers get a 50% discount on delivery fee.\n",
    "- Delivery fee is determined by each branch based on a linear relationship with distance.\n",
    "- A ratio of delivery fee to distance for customers having loyalty should be half of that for customers who do not. \n",
    "- Finding distributions for each and then reclassifying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['weekday_weekend'] = items['date']\\\n",
    ".apply(lambda x : 0 if x.weekday() <= 5 else 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_check (row):\n",
    "    if (datetime.strptime('08:00:00','%H:%M:%S').time() <= row['time'] <= datetime.strptime('12:00:00','%H:%M:%S').time()):\n",
    "        return 0\n",
    "    if (datetime.strptime('12:00:00','%H:%M:%S').time() < row['time'] <= datetime.strptime('16:00:00','%H:%M:%S').time()):\n",
    "        return 1\n",
    "    if (datetime.strptime('16:00:00','%H:%M:%S').time() < row['time'] <= datetime.strptime('20:00:00','%H:%M:%S').time()):\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['day_time'] = items.apply(time_check,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loyalty = items[items.error_found == True][['order_id','date','time','customerHasloyalty?','distance_to_customer_KM', 'branch_code','delivery_fee','weekday_weekend','day_time']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loyalty['ratio'] = df_loyalty.delivery_fee/df_loyalty.distance_to_customer_KM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loyalty.boxplot(column = 'ratio', by='customerHasloyalty?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loyalty.hist(column = 'ratio',bins = 10, by = 'customerHasloyalty?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_noLoyalty = df_loyalty[df_loyalty['customerHasloyalty?']==0].ratio.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_Loyalty = df_loyalty[df_loyalty['customerHasloyalty?']==1].ratio.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"95% of customers who do not have a loyalty status have a delivery fee to distance ratio between: \"\\\n",
    "      ,str(ratio_noLoyalty['mean']-1.96*ratio_noLoyalty['std']), \" and \",\\\n",
    "      str(ratio_noLoyalty['mean']+1.96*ratio_noLoyalty['std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"95% of customers who have a loyalty status have a delivery fee to distance ratio between: \"\\\n",
    "      ,str(ratio_Loyalty['mean']-1.96*ratio_Loyalty['std']), \" and \",\\\n",
    "      str(ratio_Loyalty['mean']+1.96*ratio_Loyalty['std']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a customer has delivery fee to distance ratio <1 and does not have a loyalty status or if a customer has delivery fee to distance ratio >1 and has a loyalty status, these records can be considered erroneous. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records that may contain error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loyalty_error = items[items.error_found == False]\\\n",
    "[['order_id','date','time','customerHasloyalty?','distance_to_customer_KM', 'branch_code','delivery_fee']]\\\n",
    ".drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loyalty_error['ratio'] = df_loyalty_error.delivery_fee/df_loyalty_error.distance_to_customer_KM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loyalty_error.boxplot(column = 'ratio', by='customerHasloyalty?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loyalty_error.hist(column = 'ratio',bins = 10, by = 'customerHasloyalty?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loyalty_error[df_loyalty_error['customerHasloyalty?']==0].ratio.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loyalty_error[df_loyalty_error['customerHasloyalty?']==1].ratio.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records containing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loyalty_error[(df_loyalty_error['customerHasloyalty?']==0) & (df_loyalty_error['ratio']<1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loyalty_error[(df_loyalty_error['customerHasloyalty?']==1) & (df_loyalty_error['ratio']>1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loyalty_err_id = []\n",
    "loyalty_err_id.append(list(df_loyalty_error[(df_loyalty_error['customerHasloyalty?']==0) \\\n",
    "                                            & (df_loyalty_error['ratio']<1)].order_id))\n",
    "loyalty_err_id.append(list(df_loyalty_error[(df_loyalty_error['customerHasloyalty?']==1) \\\n",
    "                                            & (df_loyalty_error['ratio']>1)].order_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loyalty_err_li = [item for sublist in loyalty_err_id for item in sublist]\n",
    "len(loyalty_err_li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "39 records contain errors for loyalty status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['loyalty_error'] = items.order_id.apply(lambda x: True if x in loyalty_err_li else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.error_found[items.loyalty_error == True] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing Customer Loyalty\n",
    "- This can be done by changing the loyalty status for items that contain error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_loyalty (row):\n",
    "    if row['loyalty_error']==True:\n",
    "        if row['customerHasloyalty?'] == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        return row['customerHasloyalty?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['customerHasloyalty?'] = items.apply(fix_loyalty,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  delivery_fee\n",
    "- delivery fee depends on \n",
    "    a. weekend or weekday (1 or 0) - as a continuous variable\n",
    "    b. time of the day (morning 0, afternoon 1, evening 2) - as a continuous variable\n",
    "    c. distance between branch and customer\n",
    "  and is calculated differently for different branches. \n",
    "  \n",
    "- also loyal customers get a 50% discount. \n",
    "- Method used is the same as one in Missing data. Data containing correct information from missing data is combined with data from dirty data to give more data to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[items.error_found == True].plot.scatter(x = 'delivery_fee', y = 'distance_to_customer_KM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(items.branch_code)\n",
    "items = items.join(dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data = items[items.error_found == True][['order_id','distance_to_customer_KM','customerHasloyalty?',\\\n",
    "                                               'weekday_weekend','day_time','NS','BK','TP','delivery_fee']]\\\n",
    ".drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_training_data = training_data[['order_id','distance_to_customer_KM','customerHasloyalty?',\\\n",
    "                                               'weekday_weekend','day_time','NS','BK','TP','delivery_fee']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_data = pd.concat([Train_data,missing_training_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = \\\n",
    "train_test_split(full_training_data[['distance_to_customer_KM','customerHasloyalty?','weekday_weekend','day_time','NS','BK','TP']],\\\n",
    "                 full_training_data['delivery_fee'], test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.fit(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.score(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.score(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both training and testing error are similar at 87%. The model generalises well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the amount of variation in data for correct delivery fee so we can select data that contains greater variation. 95% confidence interval is used for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_predict_known = lr_model\\\n",
    ".predict(full_training_data[['distance_to_customer_KM','customerHasloyalty?','weekday_weekend','day_time','NS','BK','TP']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_data[\"pred_delivery\"] = lr_model_predict_known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_data['deliv_diff'] = full_training_data[\"pred_delivery\"]-full_training_data[\"delivery_fee\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_var = full_training_data['deliv_diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Normal variation: \"\\\n",
    "      ,str(norm_var['mean']-1.96*norm_var['std']), \" and \",\\\n",
    "      str(norm_var['mean']+1.96*norm_var['std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_find_delivery = items[items.error_found == False]\\\n",
    "[['order_id','date','time','distance_to_customer_KM','branch_code','customerHasloyalty?',\\\n",
    "  'delivery_fee','weekday_weekend','day_time','NS','TP','BK']]\\\n",
    ".drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_find_delivery.plot.scatter(x = 'delivery_fee', y = 'distance_to_customer_KM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_predict = lr_model.predict(items_find_delivery[['distance_to_customer_KM','customerHasloyalty?',\\\n",
    "                                                         'weekday_weekend','day_time','NS','BK','TP']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_find_delivery[\"pred_delivery\"] = lr_model_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_find_delivery['deliv_diff'] = items_find_delivery[\"pred_delivery\"]-items_find_delivery[\"delivery_fee\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deliv_err_li_df = items_find_delivery[(items_find_delivery.deliv_diff > norm_var['mean']+1.96*norm_var['std']) |\\\n",
    "                   (items_find_delivery.deliv_diff < norm_var['mean']-1.96*norm_var['std'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records that contain more than normal variation in delivery fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deliv_err_li_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deliv_err_li = list(deliv_err_li_df.order_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['deliv_error'] = items.order_id.apply(lambda x: True if x in deliv_err_li else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.error_found[items.loyalty_error == True] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix Delivery Fee\n",
    "\n",
    "- orders with incorrect delivery fee are replaced with the delivery fee predicted by our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deliv_dict = dict(zip(deliv_err_li_df.order_id,deliv_err_li_df.pred_delivery))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_deliv (row):\n",
    "    if row[\"deliv_error\"] == True:\n",
    "        return deliv_dict[row[\"order_id\"]]\n",
    "    else:\n",
    "        return row[\"delivery_fee\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[\"delivery_fee\"] = items.apply(fix_deliv,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up and exporting file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.drop(['num_items','order_item','id_code','error_found','date_error','time_error','type_error','branch_error','item_error','pricing_error',\\\n",
    "           'coord_error','lon', 'lat', 'distance_to_customer_KM_test', 'dist_diff', 'dist_error', 'weekday_weekend','day_time', 'loyalty_error',\\\n",
    "           'BK', 'NS', 'TP', 'deliv_error'],axis = 1).sort_values('order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['item_quantity'] =   '('+ \"'\" + items['item'] + \"'\" + ' , ' + items['quantity'].astype(str) + ')'\n",
    "df1 = items.groupby(['order_id'])['item_quantity'].apply(','.join).to_frame().reset_index()\n",
    "df1['item_quantity'] = '[' + df1['item_quantity'] +']' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.drop_duplicates(subset=['order_id',], keep='last', inplace=False)\n",
    "dirty_data_solution = pd.merge(items,df1,on='order_id')\n",
    "dirty_data_solution = dirty_data_solution.rename(columns = {'item_quantity_y':'order_items'})\n",
    "dirty_data_solution = dirty_data_solution.drop(['item','item_quantity_x','quantity'],axis = 1).sort_values('order_id')\n",
    "dirty_data_solution = dirty_data_solution[['order_id', 'date','time','order_type','branch_code', 'order_items', 'order_price','customer_lat','customer_lon', 'customerHasloyalty?', 'distance_to_customer_KM', 'delivery_fee']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data_solution.to_csv(\"dirty_data_solution.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://www.mikulskibartosz.name/how-to-split-a-list-inside-a-dataframe-cell-into-rows-in-pandas/ website accessed on 6/10/19"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
